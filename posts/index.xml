<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Heath Harrelson&#39;s Blog</title>
    <link>https://heathharrelson.github.io/posts/</link>
    <description>Heath Harrelson&#39;s Blog (Posts)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Jul 2020 21:07:10 -0700</lastBuildDate>
    
    <atom:link href="https://heathharrelson.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Crontab Guru</title>
      <link>https://heathharrelson.github.io/posts/crontab-guru/</link>
      <pubDate>Mon, 06 Jul 2020 21:07:10 -0700</pubDate>
      
      <guid>https://heathharrelson.github.io/posts/crontab-guru/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re like me, you can never remember the exact syntax for a UNIX &lt;a href=&#34;https://en.wikipedia.org/wiki/Cron&#34;&gt;cron&lt;/a&gt; schedule. &lt;a href=&#34;https://crontab.guru/&#34;&gt;Crontab Guru&lt;/a&gt; is an excellent reference, with &lt;a href=&#34;https://crontab.guru/examples.html&#34;&gt;many examples&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Testing the Kubernetes NFS Client Provisioner</title>
      <link>https://heathharrelson.github.io/posts/nfs-client-provisioner/</link>
      <pubDate>Sat, 27 Jun 2020 16:20:41 -0700</pubDate>
      
      <guid>https://heathharrelson.github.io/posts/nfs-client-provisioner/</guid>
      <description>&lt;p&gt;While Kubernetes pods are ephemeral, most applications have &lt;em&gt;some&lt;/em&gt; state that we want to survive a restart, which means dealing with Kubernetes&amp;rsquo; &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/&#34;&gt;persistent volumes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For the bare-metal clusters I deal with at work, we manually create a subdirectory on an NFS share, then configure the relevant containers to mount the directory as a volume. Setting this up manually doesn&amp;rsquo;t feel particularly &amp;ldquo;cloud native,&amp;rdquo; so I was intrigued when I came across this &lt;a href=&#34;https://opensource.com/article/20/6/kubernetes-nfs-client-provisioning&#34;&gt;opensource.com article on creating dynamic persistent volumes using the NFS client provisioner&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The use case for the NFS client provisioner is essentially what I described above. If you have an existing NFS share, the NFS client provisioner can automatically create subdirectories on it to satisfy persistent volume claims. Sounds great! Let&amp;rsquo;s, try it, shall we?&lt;/p&gt;
&lt;p&gt;Following the steps from the &lt;a href=&#34;https://opensource.com/article/20/6/kubernetes-nfs-client-provisioning&#34;&gt;opensource.com article&lt;/a&gt;, I cloned the &lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage&#34;&gt;external storage respository from GitHub&lt;/a&gt; to my Raspberry Pi cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HypriotOS/armv7: k8s@k8smaster in ~
$ git clone https://github.com/kubernetes-incubator/external-storage.git
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I applied the access control permissions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ kubectl apply -f rbac.yaml
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I configured &lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/deploy/deployment-arm.yaml&#34;&gt;&lt;code&gt;deployment-arm.yaml&lt;/code&gt;&lt;/a&gt; for my NFS share, then deployed the NFS client provisioner by applying the manifest.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ kubectl apply -f deployment-arm.yaml
deployment.apps/nfs-client-provisioner created
HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ kubectl get pods
NAME                                     READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-cbb9b57db-rqrz6   1/1     Running   0          10m
nginx-f89759699-mz2tg                    1/1     Running   0          11d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Finally, I created the &lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/deploy/class.yaml&#34;&gt;storage class&lt;/a&gt;, the &lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/deploy/test-claim.yaml&#34;&gt;volume claim&lt;/a&gt;, and the &lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/deploy/test-pod.yaml&#34;&gt;test pod&lt;/a&gt; by applying the related manifests.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ kubectl apply -f class.yaml
storageclass.storage.k8s.io/managed-nfs-storage created
HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ kubectl get storageclass
NAME                  PROVISIONER   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
managed-nfs-storage   nfs-storage   Delete          Immediate           false                  25s
HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ kubectl apply -f test-claim.yaml
persistentvolumeclaim/test-claim created
HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ kubectl apply -f test-pod.yaml
pod/test-pod created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As expected, the NFS client provisioner created a directory for the persistent volume claim:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ ls -l /cluster-data
total 24
drwxrwxrwx 2 nobody nogroup  4096 Jun 27 17:15 default-test-claim-pvc-7a0cafd4-5958-4671-aa59-6e5a9b1086e7
drwx------ 2 root   root    16384 Jun 17 18:19 lost+found
drwxr-xr-x 3 k8s    k8s      4096 Jun 21 18:37 manifests
HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ ls -l /cluster-data/default-test-claim-pvc-7a0cafd4-5958-4671-aa59-6e5a9b1086e7/
total 0
-rw-r--r-- 1 nobody nogroup 0 Jun 27 17:15 SUCCESS
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So it definitely works. However, there are two things that concern me, one minor and one major.&lt;/p&gt;
&lt;p&gt;The minor issue is that if you aren&amp;rsquo;t careful with configuration, your data will be deleted when you delete the volume claim.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ ls /cluster-data
default-test-claim-pvc-7a0cafd4-5958-4671-aa59-6e5a9b1086e7  lost+found  manifests
HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ kubectl delete pvc test-claim
persistentvolumeclaim &amp;quot;test-claim&amp;quot; deleted
HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ ls /cluster-data
lost+found  manifests
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This probably isn&amp;rsquo;t a major problem, as long as you&amp;rsquo;re aware of it. You can also configure the storage class to archive the data when the PVC is deleted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ git diff class.yaml
diff --git a/nfs-client/deploy/class.yaml b/nfs-client/deploy/class.yaml
index 4d3b4805..ba774236 100644
--- a/nfs-client/deploy/class.yaml
+++ b/nfs-client/deploy/class.yaml
@@ -2,6 +2,6 @@ apiVersion: storage.k8s.io/v1
 kind: StorageClass
 metadata:
   name: managed-nfs-storage
-provisioner: fuseim.pri/ifs # or choose another name, must match deployment&#39;s env PROVISIONER_NAME&#39;
+provisioner: nfs-storage # or choose another name, must match deployment&#39;s env PROVISIONER_NAME&#39;
 parameters:
-  archiveOnDelete: &amp;quot;false&amp;quot;
+  archiveOnDelete: &amp;quot;true&amp;quot;
$ kubectl delete pvc test-claim
persistentvolumeclaim &amp;quot;test-claim&amp;quot; deleted
HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ ls /cluster-data
archived-default-test-claim-pvc-734bb4eb-e070-48dc-9071-a56c9ef71c6d  manifests
lost+found
HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ ls /cluster-data/archived-default-test-claim-pvc-734bb4eb-e070-48dc-9071-a56c9ef71c6d/
SUCCESS
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The more concerning issue is that the NFS client provisioner creates directories that are world writable (ðŸ˜±ðŸ˜±ðŸ˜±).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HypriotOS/armv7: k8s@k8smaster in ~/external-storage/nfs-client/deploy
$ ls -l /cluster-data
total 24
# NOOOOOOOOOOOOOOO!
drwxrwxrwx 2 nobody nogroup  4096 Jun 27 20:49 default-test-claim-pvc-f8129e20-d910-42b2-9d01-78464fa6aee0
drwx------ 2 root   root    16384 Jun 17 18:19 lost+found
drwxr-xr-x 3 k8s    k8s      4096 Jun 21 18:37 manifests
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Even worse, the &lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/cmd/nfs-client-provisioner/provisioner.go#L69&#34;&gt;directory permissions are hardcoded&lt;/a&gt; in the source.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;p&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;nfsProvisioner&lt;/span&gt;) &lt;span style=&#34;color:#a6e22e&#34;&gt;Provision&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;options&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;controller&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;VolumeOptions&lt;/span&gt;) (&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;v1&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;PersistentVolume&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt;) {
  &lt;span style=&#34;color:#75715e&#34;&gt;// ...
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
	&lt;span style=&#34;color:#a6e22e&#34;&gt;fullPath&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;filepath&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Join&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;mountPath&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;pvName&lt;/span&gt;)
	&lt;span style=&#34;color:#a6e22e&#34;&gt;glog&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;V&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).&lt;span style=&#34;color:#a6e22e&#34;&gt;Infof&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;creating path %s&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;fullPath&lt;/span&gt;)
	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;MkdirAll&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;fullPath&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0777&lt;/span&gt;); &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {
		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;errors&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;New&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;unable to create directory to provision new pv: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Error&lt;/span&gt;())
	}
	&lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Chmod&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;fullPath&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0777&lt;/span&gt;)

	&lt;span style=&#34;color:#75715e&#34;&gt;// ...
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It&amp;rsquo;s possible that I&amp;rsquo;m missing something, but this seems not great. I could definitely update the configuration to run as an unprivileged user and create directories with safer permissions, but I prefer not to maintain this kind of low-level cluster infrastructure. I suspect we&amp;rsquo;ll stick with manual volume configuration, at least until our IT organization offers a more dynamic storage solution.&lt;/p&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://opensource.com/article/20/6/kubernetes-nfs-client-provisioning&#34;&gt;Opensource.com: Provision Kubernetes NFS clients on a Raspberry Pi homelab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage&#34;&gt;GitHub: Kubernetes incubator external-storage repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Opensource.com Raspberry Pi Kubernetes Cluster Article</title>
      <link>https://heathharrelson.github.io/posts/opensource-com-raspberry-pi-kubernetes/</link>
      <pubDate>Sat, 20 Jun 2020 22:22:58 -0700</pubDate>
      
      <guid>https://heathharrelson.github.io/posts/opensource-com-raspberry-pi-kubernetes/</guid>
      <description>&lt;p&gt;As often happens, after spending several evenings getting &lt;a href=&#34;https://heathharrelson.github.io/posts/raspberry-pi-cluster-kubeadm/&#34;&gt;my Raspberry Pi 4 Kubernetes cluster&lt;/a&gt; working, I found this similar &lt;a href=&#34;https://opensource.com/article/20/6/kubernetes-raspberry-pi&#34;&gt;Opensource.com article on how to build a Raspberry Pi Kubernetes cluster&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The article is strikingly similar, although more of the setup is manual. The main difference is that the article uses Ubuntu for the Raspberry Pi&amp;rsquo;s operating system to take advantage of the availability of arm64 Docker images.&lt;/p&gt;
&lt;p&gt;A lot of my effort was spent on automating machine setup and dealing with firewall issues, so finding this might or might not have saved me any time. Still, I&amp;rsquo;m glad to see more articles on the topic, which hopefully will make it easier for others to get started on their own clusters.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://opensource.com/article/20/6/kubernetes-raspberry-pi&#34;&gt;Opensource.com: Build a Kubernetes cluster with the Raspberry Pi&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Turing Pi Kubernetes Cluster YouTube Series</title>
      <link>https://heathharrelson.github.io/posts/jeff-geerling-turing-pi-series/</link>
      <pubDate>Fri, 19 Jun 2020 23:13:07 -0700</pubDate>
      
      <guid>https://heathharrelson.github.io/posts/jeff-geerling-turing-pi-series/</guid>
      <description>&lt;p&gt;While working on building my Raspberry Pi 4 Kubernetes cluster, I came across this &lt;a href=&#34;https://www.youtube.com/playlist?list=PL2_OBreMn7Frk57NLmLheAaSSpJLLL90G&#34;&gt;excellent YouTube series from Jeff Geerling on building a 7-node Kubernetes cluster&lt;/a&gt; using a &lt;a href=&#34;https://turingpi.com/&#34;&gt;Turing Pi&lt;/a&gt; and Raspberry Pi Compute Module 3+ boards.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/kgVz4-SEhbE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The series is interesting even if you already know a bit about Kubernetes, particularly &lt;a href=&#34;https://www.youtube.com/watch?v=N4bfNefjBSw&#34;&gt;episode 3 (cluster setup)&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=IafVCHkJbtI&#34;&gt;episode 4 (deploying applications)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kgVz4-SEhbE&#34;&gt;Jeff Geerling YouTube series on Kubernetes on Raspberry Pi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/geerlingguy/turing-pi-cluster&#34;&gt;Companion GitHub repo: https://github.com/geerlingguy/turing-pi-cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rancher/k3s-ansible&#34;&gt;k3s Ansible playbook: https://github.com/rancher/k3s-ansible&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Building a Raspberry Pi Kubernetes Cluster with Kubeadm</title>
      <link>https://heathharrelson.github.io/posts/raspberry-pi-cluster-kubeadm/</link>
      <pubDate>Fri, 19 Jun 2020 22:21:38 -0700</pubDate>
      
      <guid>https://heathharrelson.github.io/posts/raspberry-pi-cluster-kubeadm/</guid>
      <description>&lt;p&gt;This article describes how to build a personal &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; cluster using &lt;a href=&#34;https://www.raspberrypi.org/&#34;&gt;Raspberry Pi&lt;/a&gt; single-board computers and &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/&#34;&gt;kubeadm&lt;/a&gt;.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;materials&#34;&gt;Materials&lt;/h2&gt;
&lt;h3 id=&#34;hardware&#34;&gt;Hardware&lt;/h3&gt;
&lt;p&gt;I used the following hardware to build the cluster described below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 Raspberry Pi 4 Model B (4 GB)&lt;/li&gt;
&lt;li&gt;3 LoveRPi PoE hat for Raspberry Pi 4 Model B (compact)&lt;/li&gt;
&lt;li&gt;TP-Link 8 port PoE gigabit switch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I previously built a cluster using Raspberry Pi 3 Model B+ computers. While this worked, the performance was underwhelming, and it didn&amp;rsquo;t leave many resources left for applications. If you have the option, the Raspberry Pi 4 is much more capable.&lt;/p&gt;
&lt;p&gt;Using &lt;a href=&#34;https://en.wikipedia.org/wiki/Power_over_Ethernet&#34;&gt;power over Ethernet (PoE)&lt;/a&gt; is optional. You can use pretty much any Ethernet switch to build your cluster, but with PoE the result is much tidier.&lt;/p&gt;
&lt;h3 id=&#34;software&#34;&gt;Software&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hypriot/image-builder-rpi/releases/tag/v1.12.2&#34;&gt;Hypriot 1.12.2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.18.3&#34;&gt;Kubernetes 1.18.3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/weaveworks/weave/releases/tag/latest_release&#34;&gt;Weave Net 2.6.5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using &lt;a href=&#34;https://blog.hypriot.com/downloads/&#34;&gt;Hypriot&lt;/a&gt; for the operating system saves installation time, because Docker is preinstalled. However, Hypriot is currently built for 32-bit armv7 architecture. The Raspberry Pi 3 and 4 can run 64-bit binaries, and plenty of the Docker containers available are only built for arm64. If that matters to you, you might want to choose a different OS.&lt;/p&gt;
&lt;h3 id=&#34;networking&#34;&gt;Networking&lt;/h3&gt;
&lt;p&gt;The Ethernet interfaces are used for cluster networking and have static IP addresses in the 10.1.1.0/24 range. WiFi interfaces are used for internet access and are configured using DHCP.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;hypriot-config&#34;&gt;Hypriot Config&lt;/h3&gt;
&lt;p&gt;Hypriot allows configuring the disk images using &lt;a href=&#34;https://cloudinit.readthedocs.io/en/20.2/index.html&#34;&gt;cloud-init&lt;/a&gt;, which reduces the manual steps required to get the cluster up and makes setup much more repeatable.&lt;/p&gt;
&lt;h4 id=&#34;user-data&#34;&gt;user-data&lt;/h4&gt;
&lt;p&gt;Following &lt;a href=&#34;https://github.com/hypriot/flash/tree/master/sample&#34;&gt;&lt;code&gt;user-data&lt;/code&gt; examples from the Hypriot GitHub repo&lt;/a&gt;, I created a &lt;a href=&#34;https://cloudinit.readthedocs.io/en/20.2/topics/examples.html#yaml-examples&#34;&gt;&lt;code&gt;user-data&lt;/code&gt; file&lt;/a&gt; for each host. See &lt;a href=&#34;https://gist.github.com/heathharrelson/ffde93b78d9a1ecd77a47e4f8c16d297&#34;&gt;this Gist&lt;/a&gt; for a full example.&lt;/p&gt;
&lt;p&gt;This &lt;code&gt;user-data&lt;/code&gt; file:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sets the hostname&lt;/li&gt;
&lt;li&gt;changes the username from &lt;code&gt;pirate&lt;/code&gt; to &lt;code&gt;k8s&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;adds an authorized SSH key&lt;/li&gt;
&lt;li&gt;adds a static &lt;code&gt;/etc/hosts&lt;/code&gt; file&lt;/li&gt;
&lt;li&gt;configures the &lt;code&gt;wlan0&lt;/code&gt; and &lt;code&gt;eth0&lt;/code&gt; interfaces&lt;/li&gt;
&lt;li&gt;sets up a workaround for issues with &lt;code&gt;iptables-nft&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;adds the Kubernetes Apt repo&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;network-config&#34;&gt;network-config&lt;/h4&gt;
&lt;p&gt;I modified &lt;code&gt;network-config&lt;/code&gt; to disable the default configuration, which enables DHCP on &lt;code&gt;eth0&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;version&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;config&lt;/span&gt;: disabled
  &lt;span style=&#34;color:#75715e&#34;&gt;# - type: physical&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;#   name: eth0&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;#   subnets:&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;#     - type: dhcp&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;h3 id=&#34;flashing-the-disk-image&#34;&gt;Flashing the disk image&lt;/h3&gt;
&lt;p&gt;Flash the disk image and cloud-init files onto SD cards with the &lt;a href=&#34;https://github.com/hypriot/flash&#34;&gt;Hypriot &lt;code&gt;flash&lt;/code&gt; tool&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ flash -d /dev/disk2 \
  -u nodeX-user-data \
  -F network-config \
  hypriotos-rpi-v1.12.2.img
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;first-boot&#34;&gt;First boot&lt;/h3&gt;
&lt;p&gt;Boot each node into Hypriot and let it resize the file system and apply &lt;code&gt;user-config&lt;/code&gt;. Once a console prompt is available, verify that &lt;code&gt;wlan0&lt;/code&gt; is up, then update and install dependencies.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt-get update
$ sudo apt-get upgrade -y
$ sudo apt-get install -y kubectl kubeadm kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Restart.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo init 6
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Verify that &lt;code&gt;wlan0&lt;/code&gt; and &lt;code&gt;eth0&lt;/code&gt; both came up. Verify that you can ping other nodes.&lt;/p&gt;
&lt;h3 id=&#34;initialize-the-cluster-with-kubeadm&#34;&gt;Initialize the cluster with kubeadm&lt;/h3&gt;
&lt;p&gt;Log into the master node and &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/&#34;&gt;install Kubernetes with kubeadm according to the documentation&lt;/a&gt;. I configured the master to advertise the API server on the &lt;code&gt;eth0&lt;/code&gt; interface. Otherwise nothing custom.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo kubeadm init --apiserver-advertise-address=10.1.1.1
...
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.1.1.1:6443 --token TOKEN_STRING \
    --discovery-token-ca-cert-hash sha256:LONG_HASH_VALUE
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After a few minutes, the control plane should be installed. Copy the &lt;code&gt;kubeadm join&lt;/code&gt; command from the output for use later.&lt;/p&gt;
&lt;p&gt;Copy the Kubeconfig file to your home directory as shown in the output and verify &lt;code&gt;kubectl&lt;/code&gt; works with it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo cp /etc/kubernetes/admin.conf /home/k8s/.kube/config
$ sudo chown k8s:k8s /home/k8s/.kube/config
$ kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;install-the-overlay-network&#34;&gt;Install the overlay network&lt;/h3&gt;
&lt;p&gt;Running &lt;code&gt;kubectl get nodes&lt;/code&gt; should show the cluster master node is in &lt;code&gt;NotReady&lt;/code&gt; state. To finish installation, install a pod overlay network. I used &lt;a href=&#34;https://github.com/weaveworks/weave&#34;&gt;Weave Net&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Kubernetes documentation Documentation currently states that Calico is the only overlay network used for e2e testing, but the Calico image is not compiled for armv7.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Download and apply the Weave Net manifest to install the overlay network.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl -L -o weave.yaml https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &#39;\n&#39;)
$ kubectl apply -f weave.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Watch the overlay pods until they enter the &amp;ldquo;Running&amp;rdquo; state.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ watch kubectl -n kube-system get pods
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The master should now be ready.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;adding-worker-nodes&#34;&gt;Adding worker nodes&lt;/h3&gt;
&lt;p&gt;On each worker node, run the &lt;code&gt;kubeadm join&lt;/code&gt; copied from &lt;code&gt;kubeadm init&lt;/code&gt; output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Example
$ sudo kubeadm join 10.1.1.1:6443 --token TOKEN_STRING \
&amp;gt;     --discovery-token-ca-cert-hash sha256:LONG_HASH_VALUE
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Watch the status of the nodes on the cluster master.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ watch kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once all the nodes are in the &amp;ldquo;Ready&amp;rdquo; state, you should have a working cluster.&lt;/p&gt;
&lt;h3 id=&#34;verify-kubernetes&#34;&gt;Verify Kubernetes&lt;/h3&gt;
&lt;p&gt;As a simple test of Kubernetes, run an Nginx pod (example from &lt;a href=&#34;https://itnext.io/building-a-kubernetes-cluster-on-raspberry-pi-and-low-end-equipment-part-1-a768359fbba3&#34;&gt;this article by Eduard Iskandarov&lt;/a&gt;). This is simpler than writing up and applying a manifest.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl create deployment nginx --image=nginx
$ kubectl create service nodeport nginx --tcp=80:80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the pod is ready, use &lt;code&gt;kubectl describe service&lt;/code&gt; to get the port number.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl describe service nginx
Name:                     nginx
Namespace:                default
Labels:                   app=nginx
Annotations:              &amp;lt;none&amp;gt;
Selector:                 app=nginx
Type:                     NodePort
IP:                       10.110.64.240
Port:                     80-80  80/TCP
TargetPort:               80/TCP
NodePort:                 80-80  32474/TCP
Endpoints:                10.44.0.1:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You should now be able to send an HTTP request to that port on any node and get a response.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Note: This is from a node *outside* the cluster, so uses the wlan0 address
$ curl http://192.168.4.42:32474/
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This should work for all nodes, including the master.&lt;/p&gt;
&lt;p&gt;Congratulations, you now have your own Kubernetes cluster!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>